{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter 7 - How to Prepare Data with Keras.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPsD+hyykaAjNogacEMTg3x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tlaNI1qmkJ3Y"},"source":["# Develop Deep Learning Models for Natural Language in Python"]},{"cell_type":"markdown","metadata":{"id":"frxsT0D2kPpU"},"source":["## 7 - How to Prepare Data with Keras"]},{"cell_type":"markdown","metadata":{"id":"WNNQNnPvkUAB"},"source":["### 7.2 - Split words with *text_to_word_sequence*"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eOsVfw_ahlNb","executionInfo":{"status":"ok","timestamp":1625906676367,"user_tz":-270,"elapsed":2,"user":{"displayName":"Keivan Ipchi Hagh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg86N33hQca1Ab2Qwir_Bu36nKxiHcT9Q3omHQcsA=s64","userId":"02569620274590613261"}},"outputId":"224f26a9-68b8-4acb-8e1e-eb0475ffeef7"},"source":["from keras.preprocessing.text import text_to_word_sequence\n","\n","text = 'The quick brown fox jumed over the lazy dog.'\n","\n","words = text_to_word_sequence(text)\n","print(words)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["['the', 'quick', 'brown', 'fox', 'jumed', 'over', 'the', 'lazy', 'dog']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0Sd9R5qwk56l"},"source":["### 7.3 - Encoding with *one_hot*"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d2p0tUUok2Hw","executionInfo":{"status":"ok","timestamp":1625906746924,"user_tz":-270,"elapsed":328,"user":{"displayName":"Keivan Ipchi Hagh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg86N33hQca1Ab2Qwir_Bu36nKxiHcT9Q3omHQcsA=s64","userId":"02569620274590613261"}},"outputId":"0a27d0d7-5120-4e04-a91c-7eb4568db6f8"},"source":["from keras.preprocessing.text import text_to_word_sequence, one_hot\n","\n","text = 'The quick brown fox jumed over the lazy dog.'\n","\n","# Get vocabulary\n","vocab = set(text_to_word_sequence(text))\n","vocab_size = len(vocab)\n","print('Vocabulary Size:', vocab_size)\n","\n","# Integer encode the vocabulary (Increased vocab_size to reduce collision)\n","encode = one_hot(text, round(vocab_size * 1.3))\n","print('one_hot Encode:', encode)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Vocabulary Size: 8\n","one_hot Encode: [8, 6, 9, 8, 4, 7, 8, 4, 8]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NWROrFakooDI"},"source":["### 7.4 - Encoding with *hashing_trick*"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FXWSRBWSlqZk","executionInfo":{"status":"ok","timestamp":1625924263863,"user_tz":-270,"elapsed":384,"user":{"displayName":"Keivan Ipchi Hagh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg86N33hQca1Ab2Qwir_Bu36nKxiHcT9Q3omHQcsA=s64","userId":"02569620274590613261"}},"outputId":"1ed400bb-c0db-467a-99b6-7cf46853095d"},"source":["from keras.preprocessing.text import text_to_word_sequence, hashing_trick\n","\n","text = 'The quick brown fox jumed over the lazy dog.'\n","\n","# Get vocabulary\n","vocab = set(text_to_word_sequence(text))\n","vocab_size = len(vocab)\n","print('Vocabulary Size:', vocab_size)\n","\n","# Integer encode the vocabulary (Increased vocab_size to reduce collision)\n","encode = hashing_trick(text, round(vocab_size * 1.3), hash_function = 'md5')\n","print('hashing_trick Encode:', encode)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Vocabulary Size: 8\n","hashing_trick Encode: [6, 4, 1, 2, 9, 5, 6, 2, 6]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gUHSbigho8_q"},"source":["### 7.5 Tokenizer API"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ugVvTYtro_70","executionInfo":{"status":"ok","timestamp":1625924587006,"user_tz":-270,"elapsed":344,"user":{"displayName":"Keivan Ipchi Hagh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg86N33hQca1Ab2Qwir_Bu36nKxiHcT9Q3omHQcsA=s64","userId":"02569620274590613261"}},"outputId":"1023b6f6-e546-464b-cbf1-c1d38c0b3f95"},"source":["from keras.preprocessing.text import Tokenizer\n","\n","docs = ['Well done!', 'Good work.', 'Great effort.', 'nice work', 'Excellent!']\n","\n","# Create tokenizer\n","t = Tokenizer()\n","\n","t.fit_on_texts(docs)\n","\n","# Summerize what was learned\n","print(t.word_counts)\n","print(t.document_count)\n","print(t.word_index)\n","print(t.word_docs)\n","\n","# Integer Encode documents\n","encoded_docs = t.texts_to_matrix(docs, mode = 'count')\n","print(encoded_docs)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n","5\n","{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n","defaultdict(<class 'int'>, {'done': 1, 'well': 1, 'good': 1, 'work': 2, 'effort': 1, 'great': 1, 'nice': 1, 'excellent': 1})\n","[[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"],"name":"stdout"}]}]}