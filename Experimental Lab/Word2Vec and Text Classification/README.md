# Word2Vec and Text Classification
Word2vec is a group of related models that data scientists use to produce word embeddings, which are numeric representations for words. For example, a word embedding lexicon may provide a 100-number vector for each word in the English dictionary. Word2vec is one such embedding.

Word2vec is implemented by shallow, two-layer neural networks that trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus assigned a corresponding vector in high dimension space. Similar words will have similar vectors in this high dimension space.
 
### References
- [Original GitHub Repo](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_11_02_word2vec.ipynb)
- [Youtube Video](https://www.youtube.com/watch?v=nWxtRlpObIs&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN&index=58&ab_channel=JeffHeaton)